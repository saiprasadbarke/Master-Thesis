\chapter{Related Work}\label{chap:relatedwork}

\paragraph{} This chapter contains a comprehensive overview of the literature on the application of deep learning-driven {\mvs} methods to the problem of Depth Estimation. A typical setup to tackle this problem involves taking multiple perspectives of a scene from different viewpoints. These images are then used as input to compute a depth map of the scene. Traditionally, the depth estimation process with {\mvs} is decomposed into three important steps: extraction of features from input images, aggregation of matching costs from features, and prediction of depth from aggregated costs. We divide our literature review into sections as per these three steps. \par 
In Section \hyperref[sec:relwork-featex]{2.1}, we examine the relevant literature for feature extraction. In Section \hyperref[sec:relwork-dispes]{2.2}, we explore the state of the art in the domain of disparity estimation and cost volume construction, which is a fundamental task for any stereo methodology. Finally, in Section \hyperref[sec:relwork-mvs]{2.3}, we present the state of the art in the domain of deep learning-driven {\mvs} methods for depth estimation. For the purpose of this review, we use the terms "disparity" and "depth" interchangeably. Depth is easily obtained as it is inversely proportional to disparity. Both depth and disparity estimation employ similar methodologies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feature Extraction}\label{sec:relwork-featex}
This is a crucial step in enhancing the fidelity of depth estimation and ensuring the robustness of the correspondences for computing the matching costs between views. Feature descriptors ensure better correspondences compared to RGB color images, particularly in regions that lack definitive texture or have repetitive patterns, and also support photometric and geometric consistency across the different views.\par
Classical methodologies such as Scale-Invariant Feature Transform (SIFT) \cite{lowe1999object}, Speeded-Up Robust Features (SURF) \cite{10.1007/11744023_32}, Oriented FAST and Rotated BRIEF (ORB) \cite{rublee2011} and neural network architectures involving convolutional neural networks (CNN) such as UNets \cite{ronneberger2015unet}, VGGs \cite{simonyan2015deep}, ResNets \cite{he2015deep} are employed to discern and characterize salient regions within images. Zbontar and LeCun \cite{Zbontar2016} introduce a deep Siamese network to compute matching costs. Feature pyramid networks (FPN) \cite{lin2017feature} are popular feature extractors that produce multiscale features that are used for downstream MVS tasks. Chang {\etal} in PSMNet \cite{chang2018pyramid} propose a pyramid stereo matching network to take advantage of global context information using spatial pyramid pooling (SPP) \cite{He_2014} and dilated convolutions \cite{yu2016multiscale} to expand receptive fields. Recently, the use of Visual Transformers (ViT) \cite{dosovitskiy2021image, cao2022mvsformer, ranftl2021vision, amir2021deep} for feature extraction gives superior results compared to the use of only FPNs or UNet. In this thesis, we implement the FPN and UNet feature extractors. We also use a pretrained DINO ViT to augment the learned features. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cost Volumes and Disparity Estimation}\label{sec:relwork-dispes}
Disparity estimation is most commonly associated with stereo vision, where two cameras capture images of the same scene from slightly different viewpoints. Disparity represents the horizontal difference in the $x$ coordinates of congruent pixels in rectified images. Image rectification simplifies the problem of calculating the 3D point coordinates in the scene to a 1D search on the same epipolar line via correspondences between two views. After the correspondences are identified between the views, the disparities are calculated methodically and transmuted into depth maps. Instrumental to this computation is the cost volume, which is a 3D or 4D data structure encapsulating matching costs that represent the similarity of a pixel's appearance in one image to its counterpart in another image across potential disparities or depths. These cost volumes, especially in contemporary deep learning-driven MVS algorithms, serve as intermediate representations, enabling neural networks to be trained for disparity or depth refinement. In an MVS setting, rectifying image pairs is not feasible because of the large angles between the images. The advantage of cost volumes is that they do not require rectified images. However, raw cost volumes contain a lot of noise due to the presence of occluded points, smoothness, reflections, noise, etc. Before regressing a depth or disparity map from these cost volumes, it, therefore, becomes necessary to perform a regularization step on them. This regularization step is performed using 3D or 2D CNN-based network architectures. We now look at the state-of-the-art in cost volume construction and regularization.\par
SGM \cite{1467526} seeks to find correspondences and estimate the disparity by minimizing an energy function that combines both data and smoothness terms. 
Dispnet \cite{Mayer2016} was the first end-to-end deep learning approach to combine feature extraction with disparity estimation. {\rmvd} uses the components of the {\dispn} architecture, mainly the correlation layer described in the article, to obtain a similarity cost between the left and right images. This correlation is based on a dot product that decimates the feature dimension from the cost volume. Pang {\etal} \cite{pang2017cascade} extend DispNet by introducing a two-stage network called cascade residual learning (CRL). The final disparity map is the sum of the disparity maps and their multiscale residuals from the first and second stages.\par 
 GC-Net \cite{kendall2017end} proposed a \textit{left-right feature concatenation} based 4D cost volume that also incorporates the feature dimension and used 3D convolutions to regularize the same. Similarly, DPSNet \cite{im2019dpsnet} also concatenates the features pairwise instead of using a similarity metric and fuses the different cost volumes simply by averaging them. Sun {\etal} in PWC-net \cite{sun2018pwc} was the first method to use pyramidal processing, warping, and cost volumes to estimate optical flow in an end-to-end manner. These principles and network architecture are extensively replicated in {\mvs}. HSMNet \cite{yang2019hierarchical} uses Volumetric Pyramid Pooling, which is an extension of SPP to decode the 4D cost volume. Guo {\etal} in GWC-Net\cite{guo2019group} used {\gwc} to form the cost volume instead of a fused representation. They also used stacked UNets, also known as a "Stacked Hourglass Configuration" to regularize the cost volume. This was first used in PSMNet \cite{chang2018pyramid} to learn better context features. CFNet by Shen {\etal} \cite{shen2021cfnet} used both fused and cascade cost volume representations to address domain differences across different datasets. GANet \cite{Zhang2019GANetGA} replaces the 3D CNN regularization with a Semi-Global Guided Aggregation (SGA) and Local Guided Aggregation (LGA) layer to reduce memory requirements. AANet \cite{xu2020aanet} used similar principles to aggregate the cost volumes by using Intra-Scale Aggregation (ISA) and Cross-Scale Aggregation (CSA).  UCS-Net \cite{cheng2020deep} used multiple small volumes called Adaptive Thin Volumes (ATV) instead of a large plane sweep volume to regress a depth map in a coarse-to-fine manner. In this thesis, we extend the {\gwc} to the {\mvs} scenario and also implement the stacked hourglass cost volume regularization network. We also explore the effects of {\gwc} in a multi-correlation layer instead of warp-only correlation and full correlation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Multi-View Stereo}\label{sec:relwork-mvs}
Multi-view stereo involves deducing the 3D structure of a viewed scene using multiple images, each with known intrinsic and extrinsic camera parameters\cite{Furukawa2015}. Structure-from-motion (SfM) algorithms\cite{Schoenberger2016SfM} are used to obtain these parameters. Conventional MVS algorithms focus on finding correspondences between reference and source images by utilizing plane sweep volumes and optimizing for photometric and geometric consistency. More advanced approaches, such as COLMAP\cite{Schoenberger2016MVS}, compute visibility information and aggregate pairwise correlations based on a probabilistic framework, where visibility and depth are alternately updated
in an iterative manner. However, approaches such as SurfaceNet\cite{Ji_2017},  LSM\cite{kar2017learning}, and RayNet\cite{paschalidou2019raynet} use a volumetric approach to estimate the geometry of the scene rather than computing a plane sweep volume. DeepMVS\cite{Huang2018} was the first method to use deep neural networks for MVS. This approach aligns the reference view with the source views using a correlation layer. This layer selects patches from the source images based on potential depth values and contrasts them with patches from the reference image. Matching attributes from different views are then fused via max-pooling. 

In comparison, MVSNet\cite{Yao2018} uses a methodology in which the source views and the reference view are compared within a learned feature environment, using the variance of the warped source features with the reference features to create the aggregated cost volume. Subsequent innovations have further refined this methodology. For instance, R-MVSNet\cite{Yao2019} enhances computational efficiency at the expense of a longer training time by employing recurrent processing of depth slices in the cost volume. Both CVP-MVSNet\cite{Yang2020} and CAS-MVSNet\cite{Gu2020} implement correlation sequentially in a cascaded manner using a coarse-to-fine approach to refine the coarse depth map and optimize computational resources and support higher resolution outputs. PVSNet \cite{Xu2020} introduced the two-view pixel-wise visibility-aware cost volume construction and aggregation and an anti-noise training strategy of using non-matching views to train the network. Meanwhile, Vis-MVSNet\cite{Zhang2020} also uses this coarse-to-fine strategy to improve the initial depth map while using the predicted uncertainty map of the scene to weigh the pixels of the predicted depth map, which is used to calculate the cost volume in the finer levels of the pipeline, thus accounting for pixel-wise visibility. All these methods use either a classification or regression approach to obtain the depth. Peng {\etal} in UniMVSNet \cite{peng2022rethinking} introduce a novel representation of costs called Unification and the Unified Focal Loss to unify the classification advantage of constraining the cost volume and the regression advantage of obtaining sub-pixel depth prediction. \par
Most models require both the minimum and maximum depth values of the scene being viewed and deliver depth predictions within this specified range. Scale Augmentation \cite{schroeppel2022benchmark} introduced in the {\rmvd} framework can be used to train the networks to be scale-agnostic and thus demonstrate better generalization results on a range of data modalities.\par


It is a significant undertaking to incorporate, train, and evaluate models that utilize all of these methodologies, some of which are beyond the scope of this thesis. This thesis focuses on implementing select methodologies alongside our baseline models as individual components to analyze their impact on network performance. Now that we have discussed the state-of-the-art, the next chapter will cover the experimental setup, baselines, datasets, and evaluation metrics for this thesis. 
