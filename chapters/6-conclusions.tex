\chapter{Discussion, Future Work and Conclusion}\label{chap:conclusion}

\section{Discussion}
The main research goal of this thesis was to implement components of the pipelines of existing {\mvs} methodologies in a modular fashion and perform an ablation study of their individual and collective effects on the performance of the model in the context of depth estimation. To achieve our goal, we developed various modules for different stages of the {\mvs} pipeline.\par

\subsection*{Changing the Feature Extraction}
 To extract features, we experimented with different versions of our baseline models, {\mvsn} and {\rmvd}, using both the Feature Pyramid Network and UNet feature extractors. We also incorporated a pretrained frozen DINO ViT into the existing {\mvsn} model. Additionally, we trained and evaluated the two baseline models with swapped feature extractors. Our findings showed that models with deeper feature extractors, such as the FPN and UNet, outperformed the baselines. However, augmenting the learned features with features from the pretrained DINO ViT produced mixed results. \par

\subsection*{Changing the Correlation Layer}
For the correlation layer, we extended the {\gwc} by Guo \etal \cite{guo2019group} and used it as a standalone correlation layer as well as augmenting the existing correlation volumes with the {\gwc} output. We saw that this worked quite well for {\mvsn} implementations. However, for {\rmvd}, which uses the Full correlation from DispNet \cite{Mayer2016}, the models performed poorly. \par

\subsection*{Changing the Cost Volume Fusion Layer}
We also experimented with different fusion methods for the individual cost volumes obtained from the correlation operation. We implemented 3D learned fusion and 3D average fusion to use with the {\mvsn} models instead of its variance fusion. For {\rmvd}, which uses a 2D learned fusion, we implemented a 3D average fusion module and trained and evaluated a model with this layer instead of the learned fusion. We observed a drop in performance for the {\mvsn} models using learned and average fusion. As its name suggests, the variance fusion layer implicitly computes the variance, i.e., the dissimilarity between the warped source views and the reference view. This is logical since {\mvsn} employs a differential homography warping as its correlation layer. Learned fusion and average fusion work well for models that compute the correlations between the reference and source features explicitly. We used this learned fusion successfully for models that use {\gwc} and the full correlation. For {\rmvd} with average fusion, we observed a slight increase in the average performance. \par

\subsection*{Changing the Cost Volume Regularization Layer}
We have incorporated two distinct cost volume regularization strategies for {\mvsn}. The first one employs the 2D DispNet decoder instead of the 3D {\mvsn} decoder. This model outperformed the baseline {\mvsn} for 128 and 256 depth levels, even though it was trained with 128 depth levels. However, this model takes a long time to train and evaluate. Additionally, we have implemented a stacked hourglass cost volume regularization unit, which performed better than its baseline. \par

\subsection*{Implementing models with a Coarse to Fine Architecture}
We also implemented the {\mvsn} model in a coarse-to-fine pattern where the depth range is updated for each stage using the coarse depth map of the previous stage. We used multilevel feature extractors, i.e., the FPN and the UNet, as feature extractors in these coarse-to-fine models. We saw a significant improvement in the case of the FPN feature extractor. However, the cascade model with the UNet feature extractor performed poorly compared to the baseline. We attribute this to the FPN generating better features than the UNet at the earlier stages of the layered network. \par

\subsection*{Changing the Training Data Properties}
We also experimented with modifying different properties of the input data and training the models on a different dataset, i.e., DTU. We saw that models trained on BlendedMVS performed significantly better than those trained on DTU. We used the Scale Augmentation technique introduced by Schr√∂ppel \etal \cite{schroeppel2022benchmark} with {\mvsn}, and the model performed significantly better than the baseline. We also removed the Scale Augmentation from {\rmvd}, where it is the default, and observed a drop in performance. We conclude that Scale Augmentation makes the trained model more robust. We next increased the number of source views for training {\mvsn} and observed an increase in performance. Next, we changed the sampling strategy for the source views for {\mvsn} and {\rmvd} instead of their default sampling strategies and observed that the All combinations - Random Selection strategy performed better than the best-matching views strategy. \par

\subsection*{Changing Model Specific Hyperparameters}
In our final study, we altered certain hyperparameters of the training setup. We changed the number of groups in {\gwc} from 4 to 32 for {\mvsn} and observed a slight increase in performance. We also changed the status of the normalization parameter for the features before computing the correlations and observed that models with normalized features being fed to the correlation module performed better than non-normalized features for all three types of correlations explored. \par


\section{Future Work}
All the above points could be further explored as future directions of research for improving the robustness of the models. Existing correlation layers can be augmented with Rank correlation and census transform to make the model more robust to illumination changes. Incorporating visibility consistency and uncertainty as feedback to the network can help it learn to deal with occlusions or merge information from multiple views to reduce uncertainty. One can also explore the direction of using GANs for adapting features generated for one distribution to another distribution to provide more robust features for the rest of the {\mvs} pipeline. This can be done using adversarial training, where the discriminator tries to distinguish between features of the source and target distributions while the generator (or an adaptation module) tries to make them indistinguishable. Using spatial or depth attention or both in the cost volume regularization unit can help the network distinguish between valid and invalid pixels or depth hypotheses by enforcing constraints on them based on the surrounding context. One could also look into different strategies for source view selection such that the occlusions are minimized.\par


\section{Conclusion}
Based on the experiments conducted, we have concluded that selecting the appropriate correlation layer to compute the similarities between the reference and source features is crucial. Choosing a feature extractor that can produce more representative features significantly enhances the model's performance. Additionally, we have found that the input data used to train the model plays a vital role in ensuring its robustness during evaluation. Simple changes, such as altering the sampling strategy for the source views, have resulted in a significant increase in performance. We also emphasize the importance of normalizing the features before correlation, as it is a straightforward approach to improve the models' performance. A coarse-to-fine architecture using multilevel features has equivalent or better performance than the single stage model with reduced memory requirement. \par
From a qualitative perspective, we have observed that {\rmvd} depth maps are comparatively smoother. We attribute this to the skip connections between the {\rmvd} cost volume regularization decoder and the feature extractor. However, both {\mvsn} and {\rmvd} models face challenges in distinguishing finer structures located further away from the camera. \par
To summarize, the research discussed in this thesis is just a small part of the many methodologies that exist for {\mvs}. Although we have assessed the impact of changing individual elements of a deep learning {\mvs} pipeline for depth estimation in isolation, there is still a lot of research required to assess the impacts of the interplay of two or more elements. A strong performance across various domains can be achieved by using the correct combination of input data, feature extractor, correlation, and cost volume regularization within the network.
    