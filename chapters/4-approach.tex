\chapter{Setup}\label{chap:approach}
\paragraph{}The main goal of this thesis is to analyze the effects of using different variations of architectural components in existing MVS models. To this end, we developed the different components in the Robust-MVD framework to be interoperable and interchangeable. Each new "combination" of architectural components represents a new model, which is first trained and then evaluated under the Robust-MVD framework. We also experiment with data augmentations and data and training setup parameters such as the number of source views, view selection strategy, number of groups in {\gwc}, etc. In this chapter, we first discuss the experimental setup that includes the baseline models, followed by the training and evaluation procedure, the datasets, metrics, the hardware, and the ablation study protocol. 


 
\section{Baseline Models}\label{sec:baseline}
Within the Robust-MVD framework, you can find the Robust MVD Baseline model \cite{schroeppel2022benchmark} designed for multiview depth estimation. This model takes advantage of existing components from FlowNet and DispNet but distinguishes itself by incorporating a unique Scale Augmentation procedure. We consider the seminal {\mvsn}\cite{Yao2018} as a baseline. We also choose {\vmn}\cite{Zhang2020} as a baseline.
\begin{enumerate}
    \item \textbf{{\mvsn}:} As a baseline, we incorporate {\mvsn}, a seminal deep learning-based model for multiview depth estimation. {\mvsn} employs a differentiable homography-based plane sweep operation, which allows it to form a cost volume with discrete depth values. This model introduces the concept of variance-based cost volume aggregation, which is crucial for handling inherent noise and redundancy in cost volumes. This cost volume fusion methodology is later used in CasMVSNet, CVP-MVSNet, VisMVSNet, and many others with some variations. All {\mvsn} derivatives and the baseline are trained with depth information from the scene. \hyperref[tab:arch-mvsn]{Table 2} shows the detailed architecture of {\mvsn}. 
    \item \textbf{{\rmvd}:} Building on the principles of {\mvsn}, the implementation and training routine of {\rmvd} extends it by adding scale-consistent depth prediction and uncertainty estimation capabilities. {\rmvd} uses components from DispNet, which is an end-to-end convolutional neural network designed to learn disparity values from stereo images. It is important to note that the parent network of {\rmvd}, i.e. DispNet, is used to estimate the disparity between a pair of Stereo Images. Consequently, {\rmvd} also builds the reference image frustum for the planesweep warping operation in the linear inverse depth space, unlike {\mvsn} and our other baseline {\vmn}, which sample the depth planes in the linear depth space. All {\rmvd} based derivatives and the baseline are trained in a fixed depth setting of $[0.4, 1000]$ meters and with scale augmentation. \hyperref[tab:arch-rmvd]{Table 3} shows the detailed architecture of {\rmvd}. 
    \item \textbf{\vmn:} We also choose {\vmn} as a baseline model, as it implements features of previous networks such as CasMVSNet \cite{Gu2020} and CVP-MVSNet \cite{Yang2020} mainly the coarse-to-fine pattern, while also incorporating pixel-wise visibility in its estimations. 
\end{enumerate}\par
\hyperref[tab:methods-comparision]{Table 1} is a taxonomy of some selected MVS methodologies. We have included the baseline models and other methods that laid the foundation for the current state-of-the-art. This chart illustrates the primary elements of a typical MVS pipeline. The rows feature various models and the distinct types of components incorporated within them.  
\input{tables/methods_comparision}

\input{tables/arch_mvsn}
\input{tables/arch_rmvd}

\section{MVS Datasets}\label{sec:datasets}
For our experiments, we use BlendedMVS for training the models. For evaluating the models we use {\kitti}, {\dtu}, {\scannet}, {\tanksandtemples} and {\ethd}. 
\begin{enumerate}
    \item \textbf{\blendedmvs}\cite{Yao2020}: This is a large-scale synthetic {\mvs} dataset tailored explicitly for deep learning applications. This dataset addresses the lack of extensive training data for learning-based MVS. Provides high-quality textured meshes from images of various scenes. These meshes are then rendered to color images and depth maps and further blended with input images to generate the training input. The data set covers more than 17k high-resolution images, covering various scenarios such as cities, architecture, sculptures, and small objects, depicting 106 training and seven validation scenes. It comprises synthetic images blended with real background textures, providing a broad range of photorealistic scenes. Being synthetic, the camera calibration is accurate enough for the purpose of MVS training. 
    \item \textbf{\dtu}\cite{Jensen2014, Aanaes2016}: The DTU Robot Image Data set is a comprehensive dataset that contains 124 scenes of various objects. The data was collected using an industrial robot-mounted structured light scanner. The scenes include a wide range of objects that were captured under different lighting conditions, enhancing the variability and complexity of the dataset. The dataset comprises 128 scans with 49 views captured in seven distinct lighting conditions. These scans are divided into 79 for training, 18 for validation, and 22 for evaluation. The large variety of scene types makes this dataset a challenging but rewarding choice for training and evaluating depth estimation models. It is a perfect dataset for the use case of 3D reconstruction
    \item \textbf{\kitti}\cite{Geiger2013, Uhrig2017}:  Developed on an autonomous driving platform, KITTI presents real-world computer vision benchmarks. It comprises tasks like stereo imaging, optical flow, visual odometry, 3D object detection, and 3D tracking. The dataset was captured using high-resolution color and grayscale video cameras in the city of Karlsruhe, Germany, in adjoining rural areas and on the highways. It includes LIDAR point clouds, GPS, and IMU data, making it one of the richest datasets for vehicle-based scene understanding in real-world settings with varying illumination conditions, large dynamic ranges, and various occlusions. 
    \item \textbf{\ethd}\cite{Schoeps2017}: This dataset addresses the limitations of existing multiview stereo benchmarks. {\ethd} contains a mix of indoor and outdoor scenes captured using a high-precision laser scanner, high-resolution DSLR images, and synchronized low-resolution stereo videos. It contains 25 high-resolution scenes and ten low-resolution scenes. It covers various viewpoints and scene types, from natural scenes to man-made indoor and outdoor environments. The high temporal and spatial resolution of the data set makes it particularly valuable for real-world applications. Considering MVS with deep learning, ETH3D is widely acknowledged as the most difficult MVS task, containing many low-textured regions such as white walls and reflective floors. This is also reflected in our evaluations, where the models perform relatively poorly on {\ethd} compared to their evaluation performance on the other test datasets. 
    \item \textbf{\tanksandtemples}\cite{Knapitsch2017}: This benchmark dataset for image-based 3D reconstruction was curated under realistic conditions with various indoor and outdoor scenes. The ground truth data for this dataset were generated using an industrial laser scanner, capturing diverse viewpoints and scene types. The challenge lies in the intricate structures it presents, making it a crucial dataset for training and testing depth estimation models.
    \item \textbf{ScanNet}\cite{Dai2017}: This RGB-D video dataset contains 2.5 million views in over 1500 scans. It includes annotations with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. ScanNet's breadth of views and annotation depth support progress in various 3D scene understanding tasks, making it a versatile choice for model evaluation.
\end{enumerate}
\subsection{Training Dataset Variants}\label{subsec:train-data-var}
For training the different models in our experiments, we define two variants of the BlendedMVS dataset. These variants differ in the number of views and the view selection strategy. 
\begin{enumerate}
    \item \textbf{BlendedMVS-MVSNet variant}: This variant of the BlendedMVS dataset contains samples that have two source views and one key view. Each view in a scene becomes the key view and the two source views are chosen that have the highest matching scores to the key view. These are selected using matching scores obtained from COLMAP\cite{Schoenberger2016MVS} This variant is as per the setting of the original {\mvsn} paper \cite{Yao2018}. This variant contains 16094 training samples in total. 
    \item  \textbf{BlendedMVS-RMVD variant}: This variant of the BlendedMVS dataset contains samples that have four source views and one key view. Each view in a scene becomes the key view and the four source views are chosen randomly from the remaining views. This variant is as per the setting of the original {\rmvd} paper \cite{schroeppel2022benchmark}. This variant contains 1774920 training samples in total.
\end{enumerate}
\subsection{Test Dataset Parameters}\label{subsec:test-dataset-params}
A concise comparison of the different attributes of the evaluation datasets is provided in \hyperref[tab:test-sets]{Table 4}. \par
For evaluating the trained models, we use the following settings for the number of views, the view selection strategy, and the spatial resolutions of the images in the test datasets:
\begin{itemize}
    \item \textbf{Number of source views \(V\)}: This is fixed to 2. We do this to limit the computational requirements during inference. 
    \item \textbf{View selection strategy}: We selected the two best-matching source images from the images available in the scene after the reference image was chosen. 
    \item \textbf{Image resolutions for test datasets}: We used the following image resolutions to evaluate the trained models. We have two sets of input resolutions. The first is as follows.
    \begin{enumerate}
        \item \textbf{\kitti}: $H=384$ and $W=1280$
        \item  \textbf{\dtu}: $H=896$ and $W=1216$
        \item \textbf{\scannet}: $H=448$ and $W=640$
        \item \textbf{\tanksandtemples}: $H=704$ and $W=1280$
        \item \textbf{\ethd}: $H=768$ and $W=1152$
    \end{enumerate}
    We use these resolutions for comparatively smaller models that leave sufficient memory on the P100 for the input data and the intermediary tensors and their operations. 
    The second set of resolutions is used to evaluate models with a significantly larger footprint that do not fit on the P100 along with the data. This is as follows.
        \begin{enumerate}
        \item \textbf{\kitti}: $H=384$ and $W=1280$
        \item  \textbf{\dtu}: $H=672$ and $W=960$
        \item \textbf{\scannet}: $H=448$ and $W=640$
        \item \textbf{\tanksandtemples}: $H=576$ and $W=960$
        \item \textbf{\ethd}: $H=576$ and $W=864$
    \end{enumerate}
    In the results tables presented in the next chapter, we mark the models evaluated using these smaller resolution settings with a \(\star\). We can see here that the spatial resolutions for {\dtu}, {\ethd}, and {\tanksandtemples} are \(0.75\) times the resolutions in the normal setting. The resolutions for {\kitti} and {\scannet} are retained. 
\end{itemize}
\input{tables/test_sets.tex} 
 
\section{Metrics}\label{sec:metrics}
\begin{itemize}
    \item \textbf{Absolute Relative Error (AbsRel)}\cite{Eigen2014, Uhrig2017, schroeppel2022benchmark}:  AbsRel is a widely used metric for depth estimation tasks. It computes the average of the absolute difference between the predicted and the ground truth depth values, normalized by the ground truth. By using absolute values, we ensure that the error is always positive, disregarding whether the estimated depth was too shallow or too deep compared to the ground truth. By taking the relative error, we account for the fact that depth perception is usually logarithmic in nature, meaning that small inaccuracies at close range are often as perceptually significant as large inaccuracies at long range. This metric, therefore, offers a balanced measure of the model's depth estimation performance, irrespective of the depth range. A lower AbsRel implies a more accurate depth estimation model. In the results section of this report, AbsRel is indicated by \textit{rel}. Absrel is calculated as:
    \begin{equation}
        rel = 100 \cdot \frac{1}{n} \sum_{i=1}^{n} \frac{1}{m} \sum_{j=1}^{m} \frac{|z_{i,j}-z^*_{i,j}|}{z^*_{i,j}}
    \end{equation}\label{eq:absrel}
    For each of the $m$ pixels with a valid depth for the ground truth, we use the index $j$. The index $i$ is used for the $n$ samples in the test set. 
    \item \textbf{Inlier Ratio} \cite{Eigen2014, Uhrig2017, schroeppel2022benchmark}: The Inlier ratio refers to the proportion of data points (or pixels, in the case of images) that fit well with the estimated model, often determined using a predefined threshold. For depth estimation, it is particularly useful for understanding the proportion of the depth map that the model accurately predicts. A high Inlier ratio suggests that the model is well-fitted to the data and performs satisfactorily. It can also indicate how robust the model is against outliers and how well it generalizes to different environments. For the purpose of this thesis, we use a threshold of 1.03. In the results section of this report, the inlier ratio is indicated by $\mathbf{\tau}$.  The inlier ratio is given by:
    \begin{equation}
         \tau = 100 \cdot \frac{1}{n} \sum_{i=1}^{n} \frac{1}{m} \sum_{j=1}^{m}[max\begin{pmatrix}\frac{z_{i,j}}{z^*_{i,j}}, \frac{z^*_{i,j}}{z_{i,j}}\end{pmatrix}<1.03]
    \end{equation}
    The notation is the same as in the Absrel equation. $[\cdot]$ denotes the Iverson bracket. The Inlier Ratio represents the percentage of pixels that have accurate predictions. A prediction is deemed correct if it has an error below 3\%.
    
    \item \textbf{Area Under the Sparsification Error Curve (AUSE)} \cite{Ilg2018}:  Uncertainty estimation is a crucial aspect of any predictive model, providing insight into the model's confidence in its predictions. For depth estimation, uncertainty can indicate regions in the scene where depth is difficult to estimate because of factors such as textureless regions, occlusions, or specular reflections. The AUSE metric measures the calibration of predicted uncertainties, i.e., whether high uncertainty predictions correspond to high errors and low uncertainty predictions to low errors.\par
    Sparsification involves retaining only a portion of the predicted pixels, based on the model's estimated uncertainty. As fewer predictions are retained (i.e., greater sparsification), ideally only keeping the most accurate predictions, the error of the retained predictions is the sparsification error. The plot of the incorrect predictions against the fraction of predictions retained is known as a sparsification plot. The area under this curve (AUSE) informs us about how well the model's confidence aligns with its accuracy.\par
    A well-calibrated model, scoring low on the AUSE, can accurately predict the depth and its own performance on each prediction. Such a model would be highly valuable in real-world applications, where understanding the reliability of predictions is critical for safe and effective decision making. A value of 0 for AUSE is considered ideal as it signifies a precise match between estimated uncertainties and real errors.
    \item \textbf{Space Complexity}: This refers to the memory required to store the model and its parameters during training. The space complexity of depth estimation models is largely determined by the complexity of the model's architecture, the resolution of the input images, and the number of views. Deep learning models, in particular, often have millions of parameters, which can take up a lot of memory. Therefore, models with high space complexity may not be feasible on devices with limited memory resources. MVS methods typically have high space complexity due to the necessity to store the cost volume. The dimensions of this 4D tensor can be immense depending on the number of views, the resolution of the images, and the number of possible depth levels considered. In addition, storing intermediate results for each pair of images also contributes to the overall space complexity. This makes it crucial to explore more memory-efficient representations or processing mechanisms, such as sparse cost volumes or cost volume compression.
    \item \textbf{Inference Time}: This measures how long it takes the model to generate output given a new input after it has been trained. For multiview depth estimation, this could depend on factors such as the number of views, the resolution of the images, the generation of cost volume, its aggregation, and finally, the depth map estimation that contributes to the overall inference time. Inference efficiency is especially important for applications that require near-real-time depth estimation, such as robotics or autonomous vehicles. Fast inference time is critical for applications that require real-time depth estimation, such as autonomous vehicles or augmented reality. 
\end{itemize}
In our experiments, the Absolute Relative Error and Inlier Ratios are computed for each of the individual test sets described in the previous section and in \hyperref[tab:test-sets]{Table 4}. Their averages, along with AUSE, model runtimes, and memory consumption, are also reported.  
\section{Hardware and Software}
The P100 GPUs are chosen for their computing prowess. The P100 has a substantial memory of nearly 16GB and high-performance CUDA cores that work with CUDA 11.8, enabling them to process the complex and resource-intensive deep learning models used in our experiments. We train and evaluate the models on a single P100 GPU. This is to eliminate complexity arising out of multi-GPU training. The {\framework} framework is implemented in PyTorch. The Anaconda environment runs Python 3.9 and PyTorch 2.0.1. 

\section{Ablation Study Protocol: Constants and Variables}\label{sec:ablation-protocol}
Our ablation study protocol systematically explores the variations and influences of different factors within the {\rmvd} framework. We carefully design our experiments to ensure a thorough investigation of each element and its collective contribution to the overall performance of the Multi-View Depth (MVD) estimation model.
The order of importance of these factors and their interactions is a critical part of our investigation. The most important are changes in the architecture of the network, followed by augmentations and properties of the input, and finally the training setup hyperparameters. During the variation of the architecture components, the other factors are kept constant. To explore the effects of input data of different modalities and the augmentations performed thereupon, we keep the architecture constant. By systematically modifying each element in our model and observing its effect on the model's performance, we can understand the impact of each factor on depth estimation, which can subsequently inform and inspire the design of more efficient and effective models. This robust approach allows us to explore the broad and intricate landscape of depth estimation. 
\subsection{Constants}

To ensure the integrity and fairness of our experiment, we maintain a constant experimental setting. We control hardware specifications by ensuring all experiments run on the same graphics card. We also maintain a consistent random seed across all of our experiments to ensure that any variations in performance can be attributed solely to the variable under investigation and not to the inherent randomness in the initialization or training process. Important hyperparameters such as the number of depth bins, batch size, initial learning rates, decay schedules of learning rates, and the resolution of the input data are also kept constant. The effect of the number of depth bins is well understood, as increasing the number of depth bins will give a more accurate output. The resolution of the input images is kept constant since the evaluation occurs on a host of different resolutions, regardless of the resolution of the input data during training. This approach also enables us to train the networks on a single GPU instead of deploying them to a multi-GPU setup, which would introduce additional complexity to the training process that needs to be taken into account.
\subsection{Variables}
\begin{enumerate}
    \item \textbf{Specific Architectural Components:} Several key architectural components form the heart of any depth estimation model. We delve into various parts, including choosing a coarse-to-fine architecture, plane sweep correlation versus {\gwc}, feature extraction network selection, and cost volume fusion method. Each component is individually altered while keeping others constant to analyze their individual and combined effects. 
    \item \textbf{Dataset Properties:} This involves varying the dataset or combinations of datasets, the number of source views, and the sampling strategy for the source views. Each of these variations is explored to understand how the properties of the data influence the performance and generalizability of the model.

    \item \textbf{Preprocessing Techniques and Image Augmentations:} We consider different types of augmentations, such as scale augmentation, light augmentations, and spatial augmentations. We modify each augmentation independently and evaluate the impact on model performance.

    \item \textbf{Training Setup Hyperparameters:} We investigate the effects of training hyperparameters, such as the number of groups in {\gwc} and normalization of features before correlation. Each hyperparameter is carefully varied while others are kept constant, and its impact on the model's performance is evaluated.
\end{enumerate}

\paragraph{}In this way, the extensive setup of the experiment, which entails carefully chosen models, diverse datasets, high-end hardware, and comprehensive performance metrics, allows for a rigorous ablation study on the {\rmvd} framework. We hope the experimental findings guide the development of more accurate, reliable, and efficient depth estimation models, fostering advancements in this field.\par
In the next chapter, we take a look at the experiments performed and their results. Based on these results, we derive some inferences into the workings of the models and their respective components as well. 

